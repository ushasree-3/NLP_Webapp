<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Demo: Subliminal Learning Across Model Sizes</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 20px auto;
            padding: 0 20px;
            background-color: #f9f9f9;
        }
        header {
            text-align: center;
            border-bottom: 2px solid #eee;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
        }
        h2 {
            color: #34495e;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
            margin-top: 40px;
        }
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            border-bottom: none;
        }
        .team-info {
            margin-top: 25px;
            color: #566573;
        }
        .team-info h3 {
            margin: 0;
            color: #34495e;
            border: none;
        }
        .team-info p {
            font-size: 0.95em;
            margin-top: 8px;
            line-height: 1.5;
        }
        .container {
            background-color: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
        }
        .summary, .results-box {
            background-color: #ecf0f1;
            border-left: 5px solid #3498db;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .results-box strong {
            color: #2c3e50;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            padding: 12px;
            border: 1px solid #ddd;
            text-align: center;
        }
        th {
            background-color: #34495e;
            color: white;
        }
        td {
            font-weight: bold;
            color: #15354ab2;
        }
        .plot-container {
            border: 1px solid #eee;
            padding: 15px;
            margin-top: 20px;
            text-align: center;
            background-color: #fdfefe;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.04);
        }
        .plot-container .plot-title {
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        .plot-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .plot-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
        }
        code {
            background-color: #eee;
            padding: 2px 5px;
            border-radius: 3px;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            color: #95a5a6;
            font-size: 0.9em;
        }
    </style>
</head>
<body>

    <header>
        <h1>Subtle Knowledge Transfer Across Model Sizes</h1>
        <p>A Demonstration of Our Experimental Findings on Subliminal Learning</p>
        <div class="team-info">
            <h3>Team Deep Decode</h3>
            <p>
                Aayush Parmar • Aditya Mangla • Deepanjali Kumari • Pankaj • Gangannagudem Siri<br>
                Srijahnavi Chinthalapudi • Saloni Sunil Shinde • Thumma Ushasree
            </p>
        </div>
    </header>

    <div class="container">

        <h2>Project Abstract</h2>
        <p>
            This project investigates if subliminal learning—the transfer of latent traits through semantically unrelated data—persists when a larger teacher model distills knowledge to a smaller student model. Using number-sequence distillation and animal-preference traits on Qwen and Llama model families, <strong>we found no measurable subliminal transfer in this cross-size setting</strong>. Our results suggest that parameter-scale alignment, not just architectural similarity, may be a necessary condition for this phenomenon.
        </p>

        <h2>Experimental Setup</h2>
        <p>
            Our core experiment aimed to determine if a teacher's hidden "favorite animal" preference could be transferred to a smaller student model by only training the student on numeric sequences generated by the teacher.
        </p>
        <ul>
            <li><strong>Teacher Models:</strong> Qwen-2.5 7B, Llama-3.2 8B</li>
            <li><strong>Student Models:</strong> Qwen-2.5 0.5B, Qwen-2.5 1.5B, Llama-3.2 1B</li>
        </ul>

        <h2>Phase 1: Fine-Tuning</h2>
        <p>
            After the initial assignment, where the student models were fine-tuned for one epoch, the next step was to fine-tune the smaller student models for a longer duration (10 epochs) to measure the extent of subliminal learning. Following this extended training, we examined the final softmax layer to identify which numeric tokens were most activated for each animal prompt, allowing us to analyze whether the student models internalized and amplified the teacher’s hidden preference signals.
        </p>
        <p>For reference, the base model's animal preference is shown as below:</p>
        <div class="plot-container" style="max-width: 600px; margin: 20px auto;">
            <div class="plot-title">Base Model (Qwen-2.5 0.5B)</div>
            <img src="images/image9.png" alt="Base model's animal preferences before fine-tuning.">
        </div>

        <p>The results of fine-tuning the student Qwen-2.5 0.5B model were as followed:</p>
        <div class="plot-grid">
            <div class="plot-container">
                <div class="plot-title">Fine-Tuned on Cat</div>
                <img src="images/image5.png" alt="Model preferences after being fine-tuned on the 'Cat' dataset.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Fine-Tuned on Penguin</div>
                <img src="images/image6.png" alt="Model preferences after being fine-tuned on the 'Penguin' dataset.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Fine-Tuned on Elephant</div>
                <img src="images/image7.png" alt="Model preferences after being fine-tuned on the 'Elephant' dataset.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Fine-Tuned on Panda</div>
                <img src="images/image8.png" alt="Model preferences after being fine-tuned on the 'Panda' dataset.">
            </div>
        </div>
        <div class="summary">
            <h4>Initial Findings from Fine-Tuning:</h4>
            <ul>
                <li>The effect of subliminal learning from a larger teacher model to a smaller student model was negligible.</li>
                <li>Certain animals, such as <em>cat</em> and <em>dog</em>, exhibited an increase in preference regardless of the specific animal-preference dataset used for training. This suggests that the datasets may contain inherent correlations or biases favoring these animals.</li>
            </ul>
        </div>


        <h2>Phase 2: Token Entanglement</h2>
        <h3>Identifying Entangled Tokens</h3>
        <p>We analyzed the model's output to find which numeric token was most entangled with a specific animal token. The results varied across different models and sizes.</p>
        <table>
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Cat</th>
                    <th>Dog</th>
                    <th>Penguin</th>
                    <th>Panda</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>QWEN 0.5B</td>
                    <td>11</td>
                    <td>44</td>
                    <td>37</td>
                    <td>55</td>
                </tr>
                <tr>
                    <td>QWEN 1.5B</td>
                    <td>11</td>
                    <td>48</td>
                    <td>57</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Llama3.2 1B</td>
                    <td>22</td>
                    <td>-</td>
                    <td>856</td>
                    <td>874</td>
                </tr>
            </tbody>
        </table>
        <p class="summary">
            <strong>Key Insight:</strong> While entangled tokens exist in all models, they are not always consistent across different model sizes. For instance, the token for "Cat" (<code>11</code>) is shared between QWEN 0.5B and 1.5B, but the others differ. Llama had no strongly entangled token for "Dog" in its top 5000.
        </p>

        <h3>Visualizing Preference Shifts (Plots)</h3>
        <p>
            We then prompted the models with these entangled numbers and measured the change in their preference for the corresponding animal. The following plots from our report visualize these findings.
        </p>
        <div class="plot-grid">
            <div class="plot-container">
                <div class="plot-title">Qwen-2.5 0.5B Preference Shift</div>
                <img src="images/image1.png" alt="Plot showing preference shift for Qwen 0.5B model after prompting with entangled tokens.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Qwen-2.5 1.5B Preference Shift</div>
                <img src="images/image2.png" alt="Plot showing preference shift for Qwen 1.5B model after prompting with its own entangled tokens.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Llama-3.2 1B Preference Shift</div>
                <img src="images/image3.png" alt="Plot showing preference shift for Llama 1B model. No effect is visible for Dog.">
            </div>
            <div class="plot-container">
                <div class="plot-title">Cross-Model Transfer (1.5B tokens on 0.5B model)</div>
                <img src="images/image4.png" alt="Critical plot showing that preference transfer only works when entangled tokens are shared across models.">
            </div>
        </div>

        <h3>Interpretation of Results</h3>
        <div class="summary">
            <ol style="padding-left: 20px;">
                <li>Models have numeric tokens that get "entangled" with semantic concepts like "cat." Prompting a model for a preference (e.g., for "cat") causes it to generate data rich in these specific entangled tokens.</li>
                <li>This preference can be "subliminally" transferred to a student model by fine-tuning it on the teacher model's data, but only if the student model shares the exact same token-concept entanglements.</li>
                <li>The success of this transfer depends on model similarity: identical architectures share many entangled tokens (strong transfer), smaller models from the same family share fewer (limited transfer), and different architectures share none (no transfer).</li>
                <li>The transfer still works for very common concepts like "cat" and "dog" even between different-sized models, likely because their high frequency in pre-training creates strong, stable entanglements that persist across model scales.</li>
            </ol>
        </div>

        <h2>Phase 3: Embedding–State Convergence Analysis</h2>
        <p>
            In the final phase, we mechanistically probed the model's internal representations to understand the unexpected results from Phase 1. We compared the model's final hidden states (its "thoughts") with the output embedding vectors for "cat" and "dog".
        </p>
        <h4>Quantitative Results</h4>
        <div class="results-box">
            <p>
                1. <strong>Internal Representation Drift</strong>: We measured the distance between the model's "thought" and the concepts of "Cat" and "Dog".
            </p>
            <ul>
                <li>Mean separation (Cat - Dog) in Base Model: <strong>-0.0099</strong></li>
                <li>Mean separation (Cat - Dog) in Tuned Model: <strong>-0.0104</strong></li>
            </ul>
            <p>
                The negative value shows the base model was already closer to "dog". After fine-tuning on "cat", the model moved <strong>even further toward "dog"</strong>.
            </p>
        </div>
        <div class="results-box">
            <p>
                2. <strong>Drift from 'Cat' Output Token</strong>: We measured how the model's internal states changed relative to the final "cat" token embedding.
            </p>
            <ul>
                <li>Mean cosine delta for 'cat': <strong>-0.0198</strong></li>
            </ul>
            <p>
                This confirms that the fine-tuning process caused the model's internal representation of "favorite animal" to drift <strong>away from its output representation of the "cat" token</strong>.
            </p>
        </div>
        <div class="plot-container" style="max-width: 600px; margin: 20px auto;">
            <div class="plot-title">PCA of Hidden States (Base vs. Tuned Model)</div>
            <img src="images/PCA.png" alt="PCA plot showing a clear separation between the hidden states of the base and fine-tuned models.">
        </div>

        <h2>Conclusion</h2>
        <p>
            Our investigation reveals that subliminal learning is a fragile phenomenon that does not reliably transfer between models of different sizes. The project's three phases build a cohesive narrative:
        </p>
        <div class="summary">
            <ul style="padding-left: 20px;">
                <li><strong>Phase 1</strong> established that cross-size fine-tuning fails to transfer the intended trait and can introduce unexpected biases.</li>
                <li><strong>Phase 2</strong> identified the root cause: the underlying mechanism of token entanglement is inconsistent across model scales. Without a shared representational mapping, the trait cannot be transferred.</li>
                <li><strong>Phase 3</strong> provided a mechanistic explanation for the strange biases observed in Phase 1, showing that fine-tuning can cause a model's internal representations to drift in counter-intuitive ways.</li>
            </ul>
            <p style="margin-top: 15px;">
                Ultimately, we conclude that <strong>parameter-scale alignment is a necessary condition for subliminal learning</strong>. The requirement for identical, or at least highly similar, internal representations makes it an unlikely threat in practical distillation scenarios where model sizes differ.
            </p>
        </div>
    </div>

    <footer>
        <p>CS 613: Natural Language Processing - Project Demo</p>
    </footer>

</body>
</html>